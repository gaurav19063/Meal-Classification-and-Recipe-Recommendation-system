{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "co-occurring_ingredients_auxiliary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtXNeex9zAHz"
      },
      "source": [
        "# **Auxiliary Code for generating frequent itemsets**\n",
        "\n",
        "### Frequent itemsets are generated using Apriori algorithm\n",
        "### Separate itemset are created based on meal type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oz9doXgBZWz"
      },
      "source": [
        "***Created by P. Akshay Kumar***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ImTDZIAyyZS"
      },
      "source": [
        "# all imports\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTdVGt-ozYj_"
      },
      "source": [
        "**Dataset load and pre-processing and then converted to transactions to give as input to the apriori algorithm and the dictionary is saved to be used in the main gui screen**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4g4FX6QzT6v"
      },
      "source": [
        "df = pd.read_csv(\"recipes.csv\")\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "breakfast_df = df[df[\"Meal\"] == \"Breakfast\"]\n",
        "lunch_df = df[df[\"Meal\"] == \"Lunch\"]\n",
        "dinner_df = df[df[\"Meal\"] == \"Dinner\"]\n",
        "\n",
        "breakfast_ingredients_lists = list(breakfast_df[\"Lookup Ingredients\"])\n",
        "for i, ingredient_list in enumerate(breakfast_ingredients_lists):\n",
        "    temp_list = ingredient_list.strip('][').split(', ')\n",
        "    temp_list = [re.sub(\"'\", \"\", t) for t in temp_list]\n",
        "    temp_list = [t.strip() for t in temp_list]\n",
        "    temp_list = [lm.lemmatize(i) for i in temp_list]\n",
        "    breakfast_ingredients_lists[i] = temp_list\n",
        "    \n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(breakfast_ingredients_lists).transform(breakfast_ingredients_lists)\n",
        "breakfast_df_new = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "breakfast_df_new.drop(\"\", inplace=True, axis=1)\n",
        "\n",
        "frequent_itemsets_breakfast = apriori(breakfast_df_new, min_support=0.005, use_colnames=True)\n",
        "frequent_itemsets_breakfast['length'] = frequent_itemsets_breakfast['itemsets'].apply(lambda x: len(x))\n",
        "frequent_itemsets_breakfast = frequent_itemsets_breakfast[(frequent_itemsets_breakfast['length'] == 2) &\n",
        "                                                          (frequent_itemsets_breakfast['support'] >= 0.005)]\n",
        "frequent_itemsets_breakfast = frequent_itemsets_breakfast.sort_values(['support'], ascending=[False])\n",
        "dbfile = open('frequent_itemsets_breakfast_pickle', 'wb')\n",
        "pickle.dump(frequent_itemsets_breakfast, dbfile)\n",
        "dbfile.close()\n",
        "print(\"Breakfast top 10 combinations with eggs\")\n",
        "print(\"----------------------------------------\")\n",
        "print(frequent_itemsets_breakfast[frequent_itemsets_breakfast['itemsets'].apply(lambda x: 'egg' in str(x))][\"itemsets\"][\n",
        "      :10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-pLkcs7z7UX"
      },
      "source": [
        "**Similarly done for lunch meal type and dinner meal type**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9_U92hJzbyW"
      },
      "source": [
        "lunch_ingredients_lists = list(lunch_df[\"Lookup Ingredients\"])\n",
        "for i, ingredient_list in enumerate(lunch_ingredients_lists):\n",
        "    temp_list = ingredient_list.strip('][').split(', ')\n",
        "    temp_list = [re.sub(\"'\", \"\", t) for t in temp_list]\n",
        "    temp_list = [t.strip() for t in temp_list]\n",
        "    temp_list = [lm.lemmatize(i) for i in temp_list]\n",
        "    lunch_ingredients_lists[i] = temp_list\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(lunch_ingredients_lists).transform(lunch_ingredients_lists)\n",
        "lunch_df_new = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "lunch_df_new.drop(\"\", inplace=True, axis=1)\n",
        "\n",
        "frequent_itemsets_lunch = apriori(lunch_df_new, min_support=0.005, use_colnames=True)\n",
        "frequent_itemsets_lunch['length'] = frequent_itemsets_lunch['itemsets'].apply(lambda x: len(x))\n",
        "frequent_itemsets_lunch = frequent_itemsets_lunch[(frequent_itemsets_lunch['length'] == 2) &\n",
        "                                                  (frequent_itemsets_lunch['support'] >= 0.005)]\n",
        "frequent_itemsets_lunch = frequent_itemsets_lunch.sort_values(['support'], ascending=[False])\n",
        "dbfile = open('frequent_itemsets_lunch_pickle', 'wb')\n",
        "pickle.dump(frequent_itemsets_lunch, dbfile)\n",
        "dbfile.close()\n",
        "print(\"Lunch top 10 combinations with eggs\")\n",
        "print(\"----------------------------------------\")\n",
        "print(frequent_itemsets_lunch[frequent_itemsets_lunch['itemsets'].apply(lambda x: 'egg' in str(x))][\"itemsets\"][:10])\n",
        "\n",
        "dinner_ingredients_lists = list(dinner_df[\"Lookup Ingredients\"])\n",
        "for i, ingredient_list in enumerate(dinner_ingredients_lists):\n",
        "    temp_list = ingredient_list.strip('][').split(', ')\n",
        "    temp_list = [re.sub(\"'\", \"\", t) for t in temp_list]\n",
        "    temp_list = [t.strip() for t in temp_list]\n",
        "    temp_list = [lm.lemmatize(i) for i in temp_list]\n",
        "    dinner_ingredients_lists[i] = temp_list\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(dinner_ingredients_lists).transform(dinner_ingredients_lists)\n",
        "dinner_df_new = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "frequent_itemsets_dinner = apriori(dinner_df_new, min_support=0.005, use_colnames=True)\n",
        "frequent_itemsets_dinner['length'] = frequent_itemsets_dinner['itemsets'].apply(lambda x: len(x))\n",
        "frequent_itemsets_dinner = frequent_itemsets_dinner[(frequent_itemsets_dinner['length'] == 2) &\n",
        "                                                    (frequent_itemsets_dinner['support'] >= 0.005)]\n",
        "frequent_itemsets_dinner = frequent_itemsets_dinner.sort_values(['support'], ascending=[False])\n",
        "dbfile = open('frequent_itemsets_dinner_pickle', 'wb')\n",
        "pickle.dump(frequent_itemsets_dinner, dbfile)\n",
        "dbfile.close()\n",
        "print(\"Dinner top 10 combinations with eggs\")\n",
        "print(\"----------------------------------------\")\n",
        "print(frequent_itemsets_dinner[frequent_itemsets_dinner['itemsets'].apply(lambda x: 'eggs' in str(x))][\"itemsets\"][:10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}